{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5W7JuESovCkharZl7bYcs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erenarkangil/RAG_for_scientific_Articles/blob/main/RAG_for_bioeconomy_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA_Z0txSv8lJ",
        "outputId": "e1550c3e-d968-4fd4-a3e9-30d027b5e583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.24.13\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu # Python 3.6-3.10 (legacy, no longer available after version 1.7.3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
    "# **Information retrieval from multiple PDF Files:**\n",

    "We need to process information from five separate PDF files. The data within these files is organized into **chunks**, and our goal is to extract relevant details from these chunks. Since each document has a unique structure (e.g., single-column, two-column layouts), it's essential to carefully choose the appropriate **chunking strategy** for each file.",
    "",
    "The documents are divided into sections like **Introduction**, **Literature Review**, **Methodology**, etc. We use common patterns for chunking, such as:",
    "- **Horizontal lines** (e.g., `/xa0`)",
    "- **Page numbers** (e.g., 'Page 2 of 20')",
    ""

],
      "metadata": {
        "id": "LRyR_iTlT9uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import fitz  # PyMuPDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Or any embedding model\n",
        "import re\n",
        "import nltk  # For NLP tasks\n",
        "\n",
        "# List of PDF filenames\n",
        "pdf_files = [\"a1.pdf\", \"a2.pdf\", \"a3.pdf\", \"a4.pdf\", \"a5.pdf\"]\n",
        "\n",
        "# Dictionary to store text of each PDF\n",
        "pdf_texts = {}\n",
        "\n",
        "# Loop through each PDF file\n",
        "for pdf_file in pdf_files:\n",
        "    # Open the PDF file\n",
        "    document = fitz.open(pdf_file)\n",
        "    text = \"\"\n",
        "\n",
        "    # Extract text from each page\n",
        "    for page_num in range(document.page_count):\n",
        "        page = document[page_num]\n",
        "        text += page.get_text()\n",
        "\n",
        "    document.close()\n",
        "\n",
        "    # Store the text in the dictionary with the filename as the key\n",
        "    pdf_texts[pdf_file] = text"
      ],
      "metadata": {
        "id": "-HHgUN6_wBPP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def chunk_text(text,cstyle):\n",
        "    # Preprocess text (e.g., normalize whitespace)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Use NLTK to tokenize sentences\n",
        "    sentences = nltk.tokenize.sent_tokenize(text)\n",
        "\n",
        "    # Initialize list to hold chunks\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    # Define patterns to match for chunking\n",
        "    patterns = [\n",
        "        r\"\\b[A-Z][a-z]+, [A-Z]\\.;?\",                            # Matches author initials, e.g., \"Smith, J.\"\n",
        "        r\"https?://[^\\s]+\",                                     # Matches URLs (e.g., DOIs, journal links)\n",
        "        r\"Received:\\s?\\d{1,2}\\s?\\w+\\s?\\d{4}\",                   # Matches \"Received: 21 October 2022\"\n",
        "        r\"Accepted:\\s?\\d{1,2}\\s?\\w+\\s?\\d{4}\",                   # Matches \"Accepted: 19 November 2022\"\n",
        "        r\"Published:\\s?\\d{1,2}\\s?\\w+\\s?\\d{4}\",                  # Matches \"Published: 22 November 2022\"\n",
        "        r\"Creative\\s?Commons\\s?Attribution\",                    # Matches copyright and licensing statements\n",
        "        r\"Publisher’s Note:\",                                   # Matches publisher notes\n",
        "        r\"Abstract:\",                                           # Matches \"Abstract:\" section header\n",
        "        r\"vol\\.\\s?\\d+|vol\\s?\\d+\",                               # Matches \"vol. 26\" or \"vol 9\"\n",
        "        r\"Page\\s?\\d+\\s?(of\\s?\\d+)?\",                            # Matches \"Page 1\" or \"Page 1 of 10\"\n",
        "        r\"\\b[0-9]{3,4}\\b\",                                      # Matches standalone page numbers (e.g., \"3554\")\n",
        "        r\"[A-Z][a-z]+ et al.;?\\s+J\\.\\s+[A-Z][a-z]+\\.\\s+[A-Z][a-z]+\\.,?\\s+vol\\.\\s?\\d+,?\\s+no\\.\\s?\\d+\",  # Matches \"Delphine et al.; J. Adv. Biol. Biotechnol., vol. 26, no. 9\"\n",
        "        r\"\\bFoods\\s+\\d{4},\\s+\\d+,\\s+\\d{4}\\b\",                   # Matches citation with \"Foods\" and year\n",
        "        r\"\\d{1,2}\\s+of\\s+\\d+\",                                  # Matches \"2 of 20\" type page numbers\n",
        "        r\"\\xa0\",                                                  # Matches non-breaking space character\n",
        "        r\"\\n{2,}\",\n",
        "    ]\n",
        "\n",
        "    patterns2 = [\n",
        "    r\"\\b(?:Introduction|Methods|Results|Discussion|Conclusion)\\b\",      # Section headers\n",
        "    r\"\\b(?:vol\\.?\\s?\\d+|page\\s?\\d+(?:\\s?of\\s?\\d+)?|\\d{3,4})\\b\",         # Volume, page, standalone numbers\n",
        "    r\"\\b(?:Creative\\s?Commons\\s?Attribution|Received|Accepted|Published|Publisher's Note)\\b\", # Metadata\n",
        "    r\"\\n{2,}\"\n",
        "                                                        # Paragraph breaks (two or more newlines)\n",
        "    ]\n",
        "\n",
        "    patterns3 = [\n",
        "    r\"\\b(?:Introduction|Methods|Results|Discussion|Conclusion)\\b\",  # Main section headers\n",
        "    r\"\\n{2,}\",  # Paragraph breaks (two or more newlines)\n",
        "    ]\n",
        "    # Compile the patterns for efficiency\n",
        "\n",
        "    if cstyle == 1:\n",
        "      compiled_patterns = [re.compile(pattern) for pattern in patterns]\n",
        "    if cstyle == 2:\n",
        "      compiled_patterns = [re.compile(pattern) for pattern in patterns2]\n",
        "    if cstyle == 3:\n",
        "      compiled_patterns = [re.compile(pattern) for pattern in patterns3]\n",
        "\n",
        "\n",
        "    for sentence in sentences:\n",
        "        current_chunk.append(sentence)\n",
        "\n",
        "        # Check if any pattern matches in the sentence\n",
        "        if (len(' '.join(current_chunk)) > 1200) or \\\n",
        "           re.search(r'\\b(?:Introduction|Methods|Results|Discussion|Conclusion)\\b', sentence, re.I) or \\\n",
        "           any(pattern.search(sentence) for pattern in compiled_patterns):\n",
        "            # Create a chunk and reset current chunk\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "\n",
        "    # Add any remaining sentences as the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# List of PDF filenames\n",
        "pdf_files = [\"a1.pdf\", \"a2.pdf\", \"a3.pdf\", \"a4.pdf\", \"a5.pdf\"]\n",
        "pdf_texts_c1 = {}\n",
        "pdf_texts_c2 = {}\n",
        "pdf_texts_c3 = {}\n",
        "\n",
        "# Loop through each PDF file\n",
        "for pdf_file in pdf_files:\n",
        "    document = fitz.open(pdf_file)\n",
        "    text = \"\"\n",
        "\n",
        "    for page_num in range(document.page_count):\n",
        "        page = document[page_num]\n",
        "        text += page.get_text()\n",
        "\n",
        "    document.close()\n",
        "\n",
        "    # Chunk the extracted text\n",
        "    pdf_texts_c1[pdf_file] = chunk_text(text,1)\n",
        "    pdf_texts_c2[pdf_file] = chunk_text(text,2)\n",
        "    pdf_texts_c3[pdf_file] = chunk_text(text,3)\n",
        "\n",
        "\n",
        "\n",
        "# Now pdf_texts dictionary contains chunked text for each PDF\n",
        "for filename, chunks in pdf_texts_c3.items():  #chunks per file created\n",
        "    print(f\"Chunks from {filename}:\")\n",
        "    #for i, chunk in enumerate(chunks):\n",
        "    #    print(f\"Chunk {i + 1}:\\n{chunk[:500]}...\\n\")  # Display first 500 characters of each chunk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tp6sm0fwbxy",
        "outputId": "d11cb778-c182-4560-96da-e3168b032068"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks from a1.pdf:\n",
            "Chunks from a2.pdf:\n",
            "Chunks from a3.pdf:\n",
            "Chunks from a4.pdf:\n",
            "Chunks from a5.pdf:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before starting to create embeddings, there are two strategies we can apply:\n ",
    "\n",
    "<h2>1. We can combine all the chunks together and generate a single set of vector embeddings.\n",
    "<h2>2. Alternatively, we can create embeddings for each PDF file separately.\n",
    "\n",
    "Afterward, I calculated the similarity between the embeddings using cosine similarity. The difference between the two approaches is negligible in this case, as we are working with only five files.\n"
      ],
      "metadata": {
        "id": "9pmDssx0VJiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_chunks = []\n",
        "\n",
        "for pdf_dict in [pdf_texts_c3]:\n",
        "    for chunks in pdf_dict.values():\n",
        "        print(len(chunks))\n",
        "        all_chunks.extend(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GphWtdjgwb04",
        "outputId": "1c0a8092-41f6-4836-c27c-965f6abbe939"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n",
            "96\n",
            "45\n",
            "65\n",
            "84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld3JhnJCxqyy",
        "outputId": "44eb55dc-0050-434c-aebc-85ee89be0d5f"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "350"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jEB-c6XH2GCF"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Choose a suitable model\n",
        "m_embeddings = model.encode(all_chunks)  # Generate embeddings for each chunk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0zFQ5ePwb34",
        "outputId": "88808fe9-c97a-4a85-a82b-947f6739f764"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimension_m= m_embeddings.shape[1]\n",
        "index_m = faiss.IndexFlatL2(dimension_m)\n",
        "index_m.add(np.array(m_embeddings))"
      ],
      "metadata": {
        "id": "3-3ZRrj8wb5o"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
   "#Once the chunks and query are embedded using a transformer model, similarity search can be performed with different metrics. The **FAISS** library typically uses **Euclidean distance** or **inner product**.\n",
    "\n",
    "- <h2>Euclidean distance** measures the straight-line distance between vectors.\n",
    "- <h2>Inner product** (dot product) measures similarity based on the alignment of vectors.\n",
    "\n",
    "When using **unit vectors**, the inner product becomes equivalent to **cosine similarity**, which compares vector directions.\n",
    "\n",
    "In our case, regardless of the metric (Euclidean or cosine), the search returned the same indices.\n"
  

      ],
      "metadata": {
        "id": "hiJtnqMbXTyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_l2 = faiss.IndexFlatL2(dimension_m)\n",
        "index_l2.add(np.array(m_embeddings))\n",
        "\n",
        "# Step 3: Inner Product (no normalization)\n",
        "index_ip = faiss.IndexFlatIP(dimension_m)\n",
        "index_ip.add(np.array(m_embeddings))\n",
        "\n",
        "# Step 4: Cosine Similarity (normalize embeddings to unit vectors first)\n",
        "# Normalize the embeddings\n",
        "normalized_embeddings = m_embeddings / np.linalg.norm(m_embeddings, axis=1, keepdims=True)\n",
        "index_cosine = faiss.IndexFlatIP(dimension_m)\n",
        "index_cosine.add(np.array(normalized_embeddings))"
      ],
      "metadata": {
        "id": "xH6DbPKcwb8g"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the High-Value Products, Materials, and Biofuels that can be obtained from Spent brewer’s yeast (SBY)\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Retrieve top 5 most similar chunks\n",
        "distances_l2, indices_l2 = index_l2.search(np.array(query_embedding), k=2)\n",
        "distances_ip, indices_ip = index_ip.search(np.array(query_embedding), k=2)\n",
        "distances_cosine, indices_cosine = index_cosine.search(np.array(query_embedding), k=2)"
      ],
      "metadata": {
        "id": "GcU6QY5Ywb-7"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(indices_cosine)\n",
        "print(indices_l2)\n",
        "print(indices_ip)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wEP0cP3wcBA",
        "outputId": "ef4deefe-7d42-48cd-a11b-6646f823f1c6"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[267 270]]\n",
            "[[267 270]]\n",
            "[[267 270]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I initially tried to retrieve the top 5, but the GPT-2 mini model has a maximum token limit of 1024. Adding more context to the prompt caused some issues. My approach here requires more careful and detailed processing, as the texts and chunks are not yet clean or optimal. Additionally, the tokenization process should be examined at the end."
      ],
      "metadata": {
        "id": "OwExOWHpauYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_text = \" \".join([all_chunks[i] for i in indices_cosine[0]])\n",
        "prompt = f\"Answer the question based on the context:\\nContext: {retrieved_text}\\nQuestion: {query}\"\n",
        "#print(prompt)\n",
        "len(prompt.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_a2l2kXwcEB",
        "outputId": "65128172-47ab-4443-ae0a-d14f7f4ee367"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#len(prompt.split()) 334 words"
      ],
      "metadata": {
        "id": "o6kANkvQ3ABy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6l-JmFB583oh"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_chunks[270]"
      ],
      "metadata": {
        "id": "goVCyyA68kMv"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "llm_model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCNagi3EwcF2",
        "outputId": "6fe55ef6-6616-4a2d-ed20-5636916b1c5b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "outputs = llm_model.generate(inputs, max_length=1024, do_sample=True, temperature=0.7)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApZ6TYsR1aII",
        "outputId": "158d4cb7-5d2b-4853-bae0-806951101b86"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[len(prompt):]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "AZRekV5K1aP3",
        "outputId": "ec6b2e63-fac4-4782-f017-2af38368c433"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' for the production of high-value products and environmental protection?\\nThere are a number of different types of SBY. Some SBY in particular contain β-glucans, which are proteins that are able to increase the rate of oxidation of a natural substance. Some SBY in particular contain β-glucans, which are proteins that are able to increase the rate of oxidation of a natural substance. Some SBY in particular contain β-glucans, which are proteins that are able to increase the rate of oxidation of a natural substance. There are two main types of SBY. The first type involves the use of a variety of compounds (such as alcohols and sugars) as a feed additive. The second type involves the use of a variety of compounds (such as alcohols and sugars) to increase the rate of oxidation of a natural substance. SBY are commonly referred to as \"high-value\" and \"low-value\" products [14, 15]. The term \"high-value\" refers to the higher value of an SBY product compared with a standard high-value product. SBY have a high-value value, as do other SBY products, but are not the same. A high-value SBY product can exceed the standard SBY feed additive by over 70%. If a high-value SBY product is consumed in a large quantity, the food additive may be required to be added to the product. If the product is used as a feed additive, the ingredients and the product must be added at the same time. These two requirements have differing levels of success, as will be discussed in the next section. Protein Synthesis As a source of SBY, proteins are made using the reaction of sugar molecules with hydrogen peroxide, which is the primary gas in the brewing process. SBY are made by adding sugars and water together. Many SBY are made using the reaction of sugars with hydrogen peroxide and hydrogen peroxide, which is the primary gas in the brewing process. SBY are made by adding sugars and water together. Many SBY are made using the reaction of sugars with hydrogen peroxide and hydrogen peroxide, which is the primary gas in the brewing process. SBY are made by adding sugars and water together. Most SBY are made using the reaction of sugars with hydrogen peroxide and hydrogen peroxide, which is the primary gas in the brewing process. In addition, most SBY can be made'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = inputs[0].tolist()\n",
        "\n",
        "# Convert each token ID back into its word or subword form\n",
        "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "# Print the list of tokens as words\n",
        "print(\"Tokenized words:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud3yXVSS8uN7",
        "outputId": "ada8e134-90e9-4ce0-f7d8-ad602fdd3ea8"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words: ['Answer', 'Ġthe', 'Ġquestion', 'Ġbased', 'Ġon', 'Ġthe', 'Ġcontext', ':', 'Ċ', 'Context', ':', 'ĠAbstract', ':', 'ĠSp', 'ent', 'Ġbrewer', 'âĢ', 'Ļ', 's', 'Ġyeast', 'Ġ(', 'SB', 'Y', ')', 'Ġis', 'Ġa', 'Ġby', 'product', 'Ġof', 'Ġthe', 'Ġbrewing', 'Ġindustry', 'Ġtraditionally', 'Ġused', 'Ġas', 'Ġa', 'Ġfeed', 'Ġadditive', ',', 'Ġalthough', 'Ġit', 'Ġcould', 'Ġhave', 'Ġmuch', 'Ġbroader', 'Ġapplications', '.', 'ĠIn', 'Ġthis', 'Ġpaper', ',', 'Ġa', 'Ġcomprehensive', 'Ġreview', 'Ġof', 'Ġval', 'or', 'ization', 'Ġof', 'ĠS', 'BY', 'Ġfor', 'Ġthe', 'Ġproduction', 'Ġof', 'Ġhigh', '-', 'value', 'Ġproducts', ',', 'Ġnew', 'Ġmaterials', ',', 'Ġand', 'Ġbio', 'fu', 'els', ',', 'Ġas', 'Ġwell', 'Ġas', 'Ġenvironmental', 'Ġapplication', ',', 'Ġis', 'Ġpresented', '.', 'ĠAn', 'Ġeconomic', 'Ġperspective', 'Ġis', 'Ġgiven', 'Ġby', 'Ġmirror', 'ing', 'Ġmarketing', 'Ġof', 'Ġconventional', 'ĠS', 'BY', 'Ġwith', 'Ġinnovative', 'Ġhigh', '-', 'value', 'Ġproducts', '.', 'ĠC', 'asc', 'ading', 'Ġutilization', 'Ġof', 'Ġï', '¬', 'ģ', 'ne', 'Ġchemicals', ',', 'Ġbio', 'fu', 'els', ',', 'Ġand', 'Ġnutrients', 'Ġsuch', 'Ġas', 'Ġproteins', ',', 'Ġcarbohydrates', ',', 'Ġand', 'Ġlip', 'ids', 'Ġreleased', 'Ġby', 'Ġvarious', 'ĠS', 'BY', 'Ġtreatments', 'Ġhas', 'Ġbeen', 'Ġproposed', 'Ġas', 'Ġa', 'Ġmeans', 'Ġto', 'Ġmaximize', 'Ġthe', 'Ġsustainable', 'Ġand', 'Ġcircular', 'Ġeconomy', '.', 'ĠKey', 'words', ':', 'Ġspent', 'Ġbrewer', 'âĢ', 'Ļ', 's', 'Ġyeast', ';', 'Ġhigh', '-', 'value', 'Ġproducts', ';', 'ĠÎ²', '-', 'gl', 'uc', 'ans', ';', 'Ġcircular', 'Ġbio', 'econom', 'y', ';', 'Ġprocess', 'Ġsustainability', 'Ġ1', '.', 'ĠIntroduction', 'ĠSp', 'ent', 'Ġbrewer', 'âĢ', 'Ļ', 's', 'Ġyeast', 'Ġ(', 'SB', 'Y', ')', 'Ġaccounts', 'Ġfor', 'Ġabout', 'Ġ15', '%', 'Ġof', 'Ġtotal', 'Ġby', 'products', 'Ġgenerated', 'Ġduring', 'Ġbeer', 'Ġproduction', 'Ġ[', '1', '].', 'ĠThe', 'Ġsearch', 'Ġcriteria', 'Ġwere', 'Ġbased', 'Ġon', 'Ġkeywords', ',', 'Ġthe', 'Ġtitle', 'Ġand', 'Ġabstract', 'Ġof', 'Ġthe', 'Ġpaper', ',', 'Ġthe', 'Ġreference', 'Ġlist', ',', 'Ġand', 'Ġthe', 'Ġyear', 'Ġthe', 'Ġpaper', 'Ġwas', 'Ġpublished', '.', 'ĠThe', 'Ġprotocol', 'Ġof', 'Ġthis', 'Ġsystematic', 'Ġsearch', 'Ġis', 'Ġdivided', 'Ġinto', 'Ġthree', 'Ġparts', '.', 'ĠIn', 'ĠSection', 'Ġ1', ',', 'Ġa', 'Ġsystematic', 'Ġreview', 'Ġof', 'Ġthe', 'Ġliterature', 'Ġwas', 'Ġconducted', 'Ġto', 'Ġcollect', 'Ġall', 'Ġrelevant', 'Ġdata', 'Ġon', 'Ġyeast', 'Ġspecies', ',', 'Ġtheir', 'Ġcharacteristics', ',', 'Ġthe', 'Ġchemical', 'Ġcomposition', 'Ġof', 'ĠS', 'BY', ',', 'Ġand', 'Ġmost', 'Ġcommon', 'Ġuse', '.', 'ĠIn', 'ĠSection', 'Ġ2', ',', 'Ġa', 'Ġsystematic', 'Ġliterature', 'Ġreview', 'Ġwas', 'Ġconducted', 'Ġto', 'Ġcollect', 'Ġinformation', 'Ġon', 'Ġthe', 'Ġpotential', 'Ġuse', 'Ġof', 'ĠS', 'BY', 'Ġin', 'Ġthe', 'Ġproduction', 'Ġof', 'Ġhigh', '-', 'value', 'Ġproducts', 'Ġand', 'Ġenvironmental', 'Ġprotection', ':', 'Ġ(', 'a', ')', 'Ġproduction', 'Ġof', 'ĠÎ²', '-', 'gl', 'uc', 'an', ',', 'Ġproteins', ',', 'Ġacids', ',', 'Ġand', 'Ġnucle', 'ot', 'ides', ';', 'Ġ(', 'b', ')', 'ĠS', 'BY', 'Ġas', 'Ġan', 'Ġagent', 'Ġfor', 'Ġencaps', 'ulation', ';', 'Ġand', 'Ġ(', 'c', ')', 'Ġproduction', 'Ġof', 'Ġbio', 'fu', 'els', 'Ġand', 'Ġwastewater', 'Ġtreatment', '.', 'ĠSection', 'Ġ3', 'Ġprovides', 'Ġan', 'Ġoverview', 'Ġof', 'Ġthe', 'Ġmarket', 'Ġand', 'Ġprices', 'Ġfor', 'Ġhigh', '-', 'value', 'ĠS', 'BY', 'Ġproducts', 'Ġrelative', 'Ġto', 'ĠS', 'BY', 'Ġfeeds', '.', 'Ġ3', '.', 'ĠBrewer', 'âĢ', 'Ļ', 's', 'ĠYe', 'ast', ':', 'ĠTypes', 'Ġand', 'ĠCharacter', 'istics', 'ĠDuring', 'Ġbeer', 'Ġproduction', ',', 'Ġyeast', 'Ġis', 'Ġused', 'Ġto', 'Ġtransform', 'Ġsimple', 'Ġsugars', 'Ġderived', 'Ġfrom', 'Ġmalt', 'Ġinto', 'Ġalcohol', '.', 'ĠIn', 'Ġbrewing', ',', 'Ġye', 'asts', 'Ġare', 'Ġconvention', 'ally', 'Ġclass', 'i', 'ï', '¬', 'ģ', 'ed', 'Ġinto', 'Ġtwo', 'Ġgroups', ':', 'Ġ(', 'a', ')', 'Ġale', ',', 'Ġi', '.', 'e', '.,', 'Ġtop', '-', 'fer', 'ment', 'ing', 'Ġ(', 'Sac', 'char', 'omy', 'ces', 'Ġcere', 'vis', 'iae', '),', 'Ġand', 'Ġ(', 'b', ')', 'Ġl', 'ager', ',', 'Ġi', '.', 'e', '.,', 'Ġbottom', '-', 'fer', 'ment', 'ing', 'Ġ(', 'Sac', 'char', 'omy', 'ces', 'Ġpastor', 'ian', 'us', 'Ġvar', '.', 'Ġcar', 'ls', 'berg', 'ensis', ')', 'Ġye', 'asts', 'Ġ[', '13', '].', 'Ċ', 'Question', ':', 'ĠWhat', 'Ġare', 'Ġthe', 'ĠHigh', '-', 'Value', 'ĠProducts', ',', 'ĠMaterials', ',', 'Ġand', 'ĠBio', 'fu', 'els', 'Ġthat', 'Ġcan', 'Ġbe', 'Ġobtained', 'Ġfrom', 'ĠSp', 'ent', 'Ġbrewer', 'âĢ', 'Ļ', 's', 'Ġyeast', 'Ġ(', 'SB', 'Y', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd6o3Mvc9pjJ",
        "outputId": "ee298b77-0c28-4db2-d9fa-851770fb081d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XW89UvhO9pVg"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MT7GioPFANuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5MvSkt1ANcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the High-Value Products, Materials, and Biofuels that can be obtained from Spent brewer’s yeast (SBY)?\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Retrieve top 5 most similar chunks\n",
        "distances_l2, indices_l2 = index_l2.search(np.array(query_embedding), k=3)\n",
        "distances_ip, indices_ip = index_ip.search(np.array(query_embedding), k=3)\n",
        "distances_cosine, indices_cosine = index_cosine.search(np.array(query_embedding), k=2)\n",
        "\n",
        "retrieved_text = \" \".join([all_chunks[i] for i in indices_cosine[0]])\n",
        "prompt = f\"Answer the question based on the context:\\nContext: {retrieved_text}\\nQuestion: {query}\"\n",
        "#print(prompt)\n",
        "len(prompt.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2z8eNXH1aTG",
        "outputId": "e27b56f8-752e-4829-8e50-6e1c8f71207b"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBYWoH17BYbm"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "outputs = llm_model.generate(inputs, max_length=1024, do_sample=True, temperature=0.7)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH67zyhW_y6A",
        "outputId": "37d512e6-51cd-4c25-80c8-d604a3c5e3ad"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    
    {
      "cell_type": "markdown",
      "source": [
        "Some examples:\n",
        "# Question: What are the High-Value Products, Materials, and Biofuels that can be obtained from Spent brewer’s yeast (SBY)?\n",
        "\n",
        "\n",
        "for the production of high-value products and environmental protection?\\nThere are a number of different types of SBY. Some SBY in particular contain β-glucans, which are proteins that are able to increase the rate of oxidation of a natural substance. Some SBY in particular contain β-glucans, which are proteins that are able to increase the rate of oxidation of a natural substance. Some SBY in particular contain β-glucans, which are proteins that are able to increase the rate of oxidation of a natural substance. There are two main types of SBY. The first type involves the use of a variety of compounds (such as alcohols and sugars) as a feed additive. The second type involves the use of a variety of compounds (such as alcohols and sugars) to increase the rate of oxidation of a natural substance. SBY are commonly referred to as \"high-value\" and \"low-value\" products [14, 15]. The term \"high-value\" refers to the higher value of an SBY product compared with a standard high-value product. SBY have a high-value value, as do other SBY products, but are not the same. A high-value SBY product can exceed the standard SBY feed additive by over 70%. If a high-value SBY product is consumed in a large quantity, the food additive may be required to be added to the product. If the product is used as a feed additive, the ingredients and the product must be added at the same time. These two requirements have differing levels of success, as will be discussed in the next section. Protein Synthesis As a source of SBY, proteins are made using the reaction of sugar molecules with hydrogen peroxide, which is the primary gas in the brewing process. SBY are made by adding sugars and water together. Many SBY are made using the reaction of sugars with hydrogen peroxide and hydrogen peroxide, which is the primary gas in the brewing process. SBY are made by adding sugars and water together. Many SBY are made using the reaction of sugars with hydrogen peroxide and hydrogen peroxide, which is the primary gas in the brewing process. SBY are made by adding sugars and water together. Most SBY are made using the reaction of sugars with hydrogen peroxide and hydrogen peroxide, which is the primary gas in the brewing process. In addition, most SBY can be mad\n",
        "\n",
        "\n",
        "\n",
        "# Question:What is the CAGR of spent yeast?\n",
        "\n",
        "This question has been addressed at the Annual Meeting of the Society of Microorganisms of the International Society of Microbiology.\\n[1] Zeko-Pivaˇc. The concept of the CAGR of spent yeast is a reference to the theory of the growth of the yeast, which is a basic theory of the field. It is understood to be based on the concept of the rate of growth of yeast and their ability to ferment a given amount of substrates. The term \"spent\" is a derivation of the word \"spore\". In the literature, a yeast is often defined as the capacity for growth of the material that is produced by the active part of the structure (e.g. a cell wall, a water-soluble substrate, etc.).\n",
        "\n",
        "# Question:What is the cost to produce one gram of Mannose? What is the highest yield of Mannose that could be obtained from spent yeast?\"\n",
        "\n",
        "The yield of the malt of Th 16 24 32 51, which is 1.5 ± 0.4% of the yield of Th 16 24 32 51. This is the highest yield of mannose in the malt industry worldwide and the world's highest yield of mannose; its higher yield is due to the higher level of extract and the more complex chemical reactions that occur in the process. It is important to note that TH is a volatile organic compound, and the highest TH yield of a malt product is based on the fact that the product is the only source of high yield mannose. The highest yield of mannose in mannose is derived from the extraction of TH. There are many possible reasons to follow this reasoning. First of all, the process is very expensive and the extraction temperature is higher than the boiling temperature of mannose, and therefore the higher extraction temperature of mannose also provides a great amount of potential for the synthesis of other mannose compounds. For example, we can obtain the highest yield of Mannose in a single malt at a boiling temperature of 1.5°C, thus obtaining the high yield of mannose at a boiling temperature of 1.5°C. But the highest yield of mannose could not be obtained by the boiling temperature of mannose, as it is highly difficult to extract the highest yield of Mannose from the malt. Secondly, the process requires more energy than the other methods of extraction, and the process is not as efficient as the other methods. Thirdly, the process is very expensive and the extraction temperature is higher than the boiling temperature of mannose. This means that mannose is not a viable source of mannose, and the higher TH yield of mannose is therefore the product of the process. Finally, the process is more complex and requires more energy than the other methods. Because the process is more complex, the time required to produce the highest yield of mannose is reduced. The cost to produce the highest yield from Th 36 24 32 51, which is 1.5 ± 0.4% of the yield of Th 16 24 32 51 is the very highest. The process costs about 1.25-1.5 years of raw mannose, and the higher TH yield of Th 36 24 32 51, this is due to the higher extraction temperature of\n"
      ],
      "metadata": {
        "id": "z_gvldVxcHn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall, the responses are not best quality, but considering gpt2 is very limited the retrieval process and RAG seems to work and answer questions correctly to some degree. Please notice the performance can be improved by filtering and re-ranking and by more careful data processing + chunking"
      ],
      "metadata": {
        "id": "ctIYgZAybqqF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "elofr_IX_9JF"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Smw0z1-31aWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the CAGR of spent yeast?\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Retrieve top 5 most similar chunks\n",
        "distances_l2, indices_l2 = index_l2.search(np.array(query_embedding), k=2)\n",
        "distances_ip, indices_ip = index_ip.search(np.array(query_embedding), k=3)\n",
        "distances_cosine, indices_cosine = index_cosine.search(np.array(query_embedding), k=3)\n",
        "\n",
        "retrieved_text = \" \".join([all_chunks[i] for i in indices_l2[0]])\n",
        "prompt = f\"Answer the question based on the context:\\nContext: {retrieved_text}\\nQuestion: {query}\"\n",
        "#print(prompt)\n",
        "len(prompt.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg_M_8QhwcJG",
        "outputId": "18ca4e04-8569-411f-f6b3-a7322f81d6f7"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "401"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "outputs = llm_model.generate(inputs, max_length=1024, do_sample=True, temperature=0.7)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9pKGr2dwcLF",
        "outputId": "3c0d0bf2-12e1-4874-902f-3c1a1aeae8aa"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[len(prompt):]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "X23TBDr-wcN8",
        "outputId": "dc671c14-19b0-41d3-92a9-ac976f13345e"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'his question has been addressed at the Annual Meeting of the Society of Microorganisms of the International Society of Microbiology.\\n[1] Zeko-Pivaˇc. The concept of the CAGR of spent yeast is a reference to the theory of the growth of the yeast, which is a basic theory of the field. It is understood to be based on the concept of the rate of growth of yeast and their ability to ferment a given amount of substrates. The term \"spent\" is a derivation of the word \"spore\". In the literature, a yeast is often defined as the capacity for growth of the material that is produced by the active part of the structure (e.g. a cell wall, a water-soluble substrate, etc.).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the cost to produce one gram of Mannose? What is the highest yield of Mannose that could be obtained from spent yeast?\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Retrieve top 5 most similar chunks\n",
        "distances_ip, indices_ip = index_ip.search(np.array(query_embedding), k=2)\n",
        "distances_cosine, indices_cosine = index_cosine.search(np.array(query_embedding), k=2)\n",
        "\n",
        "retrieved_text = \" \".join([all_chunks[i] for i in indices_l2[0]])\n",
        "prompt = f\"Answer the question based on the context:\\nContext: {retrieved_text}\\nQuestion: {query}\"\n",
        "#print(prompt)\n",
        "len(prompt.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8KYBtiqwcQV",
        "outputId": "771a2852-6777-41be-9070-12c48b171de1"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "370"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
{
      "cell_type": "code",
      "source": [
        "response[len(prompt):]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "b72Isvxz_zDt",
        "outputId": "8ddf3530-5cb4-4c9a-a7c1-8ee10b0b54da"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nStructure of the synthesis of high-value products: The synthesis of SBY for the production of high-value products is a process that uses a complex of chemicals and materials. The synthesis of the SBY in yeast is achieved by the following three processes: (a) the synthesis of the SBY into the base of the complex; (b) the synthesis of the SBY into ethanol and ethanol-derived compounds; and (c) the synthesis into the base of the complex. The synthesis of the SBY is a process of the synthesis of a mixture of SBY and ethanol, and then the synthesis of ethanol and ethanol-derived compounds. The synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. The synthesis of the SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. The synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of the SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. The synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of the SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of a mixture of the two molecules that are present in the complex. Synthesis of SBY is a process of the synthesis of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "outputs = llm_model.generate(inputs, max_length=1024, do_sample=True, temperature=0.7)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIupeUBdwcTD",
        "outputId": "cb5f2a46-8345-46ce-877a-b9717e525b83"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[len(prompt):]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "JW956v7-wcVr",
        "outputId": "252cf895-ce69-4591-9452-ea59b734ca37"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Answer: The yield of the malt of Th 16 24 32 51, which is 1.5 ± 0.4% of the yield of Th 16 24 32 51. This is the highest yield of mannose in the malt industry worldwide and the world's highest yield of mannose; its higher yield is due to the higher level of extract and the more complex chemical reactions that occur in the process. It is important to note that TH is a volatile organic compound, and the highest TH yield of a malt product is based on the fact that the product is the only source of high yield mannose. The highest yield of mannose in mannose is derived from the extraction of TH. There are many possible reasons to follow this reasoning. First of all, the process is very expensive and the extraction temperature is higher than the boiling temperature of mannose, and therefore the higher extraction temperature of mannose also provides a great amount of potential for the synthesis of other mannose compounds. For example, we can obtain the highest yield of Mannose in a single malt at a boiling temperature of 1.5°C, thus obtaining the high yield of mannose at a boiling temperature of 1.5°C. But the highest yield of mannose could not be obtained by the boiling temperature of mannose, as it is highly difficult to extract the highest yield of Mannose from the malt. Secondly, the process requires more energy than the other methods of extraction, and the process is not as efficient as the other methods. Thirdly, the process is very expensive and the extraction temperature is higher than the boiling temperature of mannose. This means that mannose is not a viable source of mannose, and the higher TH yield of mannose is therefore the product of the process. Finally, the process is more complex and requires more energy than the other methods. Because the process is more complex, the time required to produce the highest yield of mannose is reduced. The cost to produce the highest yield from Th 36 24 32 51, which is 1.5 ± 0.4% of the yield of Th 16 24 32 51 is the very highest. The process costs about 1.25-1.5 years of raw mannose, and the higher TH yield of Th 36 24 32 51, this is due to the higher extraction temperature of th\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3aYox6lwcYX"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l4FOZfGlwcam"
      },
      "execution_count": 73,
      "outputs": []
    }
  ]
}
